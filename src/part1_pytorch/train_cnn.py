"""
CNNè®­ç»ƒæ¨¡å— - å¢å¼ºç‰ˆ
æ·»åŠ : Early Stopping + ç±»åˆ«æƒé‡ + æ›´å¼ºæ­£åˆ™åŒ–
"""
import torch
import torch.nn as nn
from tqdm import tqdm
import matplotlib.pyplot as plt
from config import *
from dataset import load_data, add_noise
from models import Autoencoder, CNN


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA, mode='max'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0
    
    def __call__(self, score, epoch):
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            return False
        
        if self.mode == 'min':
            improved = score < self.best_score - self.min_delta
        else:
            improved = score > self.best_score + self.min_delta
        
        if improved:
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                print(f"\nâ¹ï¸  Early Stopping! æœ€ä½³Epoch: {self.best_epoch+1}, æœ€ä½³Acc: {self.best_score:.2f}%")
        
        return self.early_stop


def train_cnn(train_loader, test_loader, autoencoder, class_weights):
    """è®­ç»ƒCNNåˆ†ç±»å™¨"""
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒCNNåˆ†ç±»å™¨ (å¢å¼ºç‰ˆ)")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = CNN().to(DEVICE)
    autoencoder.eval()  # è‡ªç¼–ç å™¨è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    
    # ä½¿ç”¨ç±»åˆ«æƒé‡çš„äº¤å‰ç†µæŸå¤±
    class_weights = class_weights.to(DEVICE)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    print(f"âš–ï¸  ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights.tolist()}")
    
    # ä¼˜åŒ–å™¨ + L2æ­£åˆ™åŒ–
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=CNN_LR, 
        weight_decay=1e-4  # L2æ­£åˆ™åŒ–
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦ - ä½™å¼¦é€€ç«
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    # Early Stopping
    early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA, mode='max')
    
    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    history = {
        'train_loss': [], 'train_acc': [],
        'test_loss': [], 'test_acc': []
    }
    
    best_acc = 0.0
    
    for epoch in range(CNN_EPOCHS):
        # ==================== è®­ç»ƒé˜¶æ®µ ====================
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:02d}/{CNN_EPOCHS}")
        for data, label in pbar:
            data, label = data.to(DEVICE), label.to(DEVICE)
            
            # æ·»åŠ å™ªå£°å¹¶é€šè¿‡è‡ªç¼–ç å™¨å»å™ª
            noisy_data = add_noise(data)
            with torch.no_grad():
                denoised_data = autoencoder(noisy_data)
            
            optimizer.zero_grad()
            output = model(denoised_data)
            loss = criterion(output, label)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(label.view_as(pred)).sum().item()
            train_total += label.size(0)
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100. * train_correct / train_total:.1f}%'
            })
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)
        
        # ==================== æµ‹è¯•é˜¶æ®µ ====================
        model.eval()
        test_loss = 0.0
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for data, label in test_loader:
                data, label = data.to(DEVICE), label.to(DEVICE)
                
                # æµ‹è¯•é›†å·²æœ‰å™ªå£°ï¼Œç›´æ¥é€šè¿‡è‡ªç¼–ç å™¨
                denoised_data = autoencoder(data)
                output = model(denoised_data)
                
                test_loss += criterion(output, label).item()
                pred = output.argmax(dim=1, keepdim=True)
                test_correct += pred.eq(label.view_as(pred)).sum().item()
                test_total += label.size(0)
        
        avg_test_loss = test_loss / len(test_loader)
        test_acc = 100. * test_correct / test_total
        history['test_loss'].append(avg_test_loss)
        history['test_acc'].append(test_acc)
        
        # æ‰“å°ç»“æœ
        current_lr = optimizer.param_groups[0]['lr']
        print(f"[CNN] Epoch {epoch+1:02d}/{CNN_EPOCHS} | "
              f"Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.2f}% | "
              f"Test Loss: {avg_test_loss:.4f} Acc: {test_acc:.2f}% | "
              f"LR: {current_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), MODEL_DIR / "cnn_best.pth")
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (acc: {best_acc:.2f}%)")
        
        # Early Stoppingæ£€æŸ¥
        if early_stopping(test_acc, epoch):
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), MODEL_DIR / "cnn_final.pth")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹ç”¨äºåç»­è¯„ä¼°
    model.load_state_dict(torch.load(MODEL_DIR / "cnn_best.pth"))
    print(f"\nâœ… CNNè®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}% (Epoch {early_stopping.best_epoch+1})")
    print(f"ğŸ“¦ å·²åŠ è½½æœ€ä½³æ¨¡å‹ç”¨äºè¯„ä¼°")
    
    return model, history


def plot_cnn_history(history):
    """ç»˜åˆ¶CNNè®­ç»ƒå†å²"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    # Lossæ›²çº¿
    axes[0].plot(epochs, history['train_loss'], 'b-o', label='Train Loss', markersize=4)
    axes[0].plot(epochs, history['test_loss'], 'r-o', label='Test Loss', markersize=4)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Cross Entropy Loss', fontsize=12)
    axes[0].set_title('CNN Training Loss', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, linestyle='--', alpha=0.7)
    
    # Accuracyæ›²çº¿
    axes[1].plot(epochs, history['train_acc'], 'b-o', label='Train Acc', markersize=4)
    axes[1].plot(epochs, history['test_acc'], 'r-o', label='Test Acc', markersize=4)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy (%)', fontsize=12)
    axes[1].set_title('CNN Training Accuracy', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, linestyle='--', alpha=0.7)
    axes[1].set_ylim([0, 105])
    
    # æ ‡æ³¨æœ€ä½³ç‚¹
    best_idx = history['test_acc'].index(max(history['test_acc']))
    axes[1].annotate(f'Best: {history["test_acc"][best_idx]:.1f}%',
                    xy=(best_idx + 1, history['test_acc'][best_idx]),
                    xytext=(best_idx + 3, history['test_acc'][best_idx] - 10),
                    arrowprops=dict(arrowstyle='->', color='red'),
                    fontsize=10, color='red')
    
    # æ·»åŠ æ—©åœæ ‡è®°
    axes[1].axvline(x=best_idx + 1, color='green', linestyle='--', alpha=0.7, label='Best Epoch')
    
    plt.tight_layout()
    plt.savefig(FIGURE_DIR / "cnn_training_history.png", dpi=150, bbox_inches='tight')
    plt.show()
    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {FIGURE_DIR / 'cnn_training_history.png'}")


if __name__ == "__main__":
    print_config()
    
    # åŠ è½½æ•°æ®
    train_loader, test_loader, _, _, class_weights = load_data()
    
    # åŠ è½½é¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨
    autoencoder = Autoencoder().to(DEVICE)
    autoencoder.load_state_dict(torch.load(MODEL_DIR / "autoencoder_best.pth"))
    print("âœ… å·²åŠ è½½é¢„è®­ç»ƒè‡ªç¼–ç å™¨")
    
    # è®­ç»ƒCNN
    model, history = train_cnn(train_loader, test_loader, autoencoder, class_weights)
    plot_cnn_history(history)